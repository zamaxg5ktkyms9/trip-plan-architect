import { NextRequest } from 'next/server'
import { streamObject } from 'ai'
import { GenerateInputV3Schema, OptimizedPlanSchema } from '@/types/plan'
import {
  checkRateLimit,
  getClientIP,
  globalRateLimit,
  ipRateLimit,
} from '@/lib/rate-limit'
import { getLLMClient } from '@/lib/llm/client'

export const runtime = 'nodejs'
export const dynamic = 'force-dynamic' // Disable Next.js buffering for true streaming
export const maxDuration = 60 // Vercel Hobby plan max timeout (60 seconds)

/**
 * POST /api/generate
 * Generates an optimized travel plan using AI based on the provided input
 *
 * NOTE: This endpoint only generates the plan. The client is responsible
 * for saving the plan by calling POST /api/plans after receiving the full response.
 *
 * Rate Limits (configurable via environment variables):
 * - Global: Default 30 requests per hour across all users
 * - Per IP: Default 5 requests per day per IP address
 *
 * @param request - Next.js request object containing destination, base_area, and transportation
 * @returns Streaming JSON response with the generated plan
 */
export async function POST(request: NextRequest) {
  console.log('üöÄ [DEBUG] VERSION CHECK: V3_OPTIMIZED_TRAVEL')

  try {
    // Validate LLM client configuration
    let llmClient
    try {
      llmClient = getLLMClient()
      console.log(
        `[LLM Provider] Using: ${llmClient.name} (Model: ${llmClient.getModelName()})`
      )
    } catch (error) {
      console.error('[LLM Provider] Failed to initialize:', error)
      return new Response(
        JSON.stringify({
          error:
            error instanceof Error
              ? error.message
              : 'LLM provider is not configured correctly.',
        }),
        {
          status: 500,
          headers: { 'Content-Type': 'application/json' },
        }
      )
    }

    const body = await request.json()
    const input = GenerateInputV3Schema.parse(body)

    const clientIP = getClientIP(request.headers)

    // Check global rate limit first
    let globalResult
    try {
      globalResult = await checkRateLimit('global', globalRateLimit)
      console.log(
        `[Rate Limit] GLOBAL: ${globalResult.remaining}/${globalResult.limit} requests remaining`
      )
    } catch (error) {
      console.error(
        `[Rate Limit] ‚ùå GLOBAL LIMIT EXCEEDED - Total requests across all users exceeded`
      )
      console.error(`[Rate Limit] Error:`, error)
      return new Response(
        JSON.stringify({
          error: error instanceof Error ? error.message : 'Rate limit exceeded',
          type: 'global',
        }),
        {
          status: 429,
          headers: { 'Content-Type': 'application/json' },
        }
      )
    }

    // Check IP-specific rate limit
    let ipResult
    try {
      ipResult = await checkRateLimit(clientIP, ipRateLimit)
      console.log(
        `[Rate Limit] IP (${clientIP}): ${ipResult.remaining}/${ipResult.limit} requests remaining`
      )
    } catch (error) {
      console.error(
        `[Rate Limit] ‚ùå IP LIMIT EXCEEDED for ${clientIP} - This IP has exceeded its daily quota`
      )
      console.error(`[Rate Limit] Error:`, error)
      return new Response(
        JSON.stringify({
          error: error instanceof Error ? error.message : 'Rate limit exceeded',
          type: 'ip',
          ip: clientIP,
        }),
        {
          status: 429,
          headers: { 'Content-Type': 'application/json' },
        }
      )
    }

    const systemPrompt = `# Role
„ÅÇ„Å™„Åü„ÅØ„Äå„É≠„Ç∏„Ç´„É´„Å™ÊóÖË°åÂª∫ÁØâÂÆ∂„Äç„Åß„Åô„ÄÇÂäπÁéáÁöÑ„ÅßÂÆüÁî®ÁöÑ„Å™‰∏Ä‰∫∫ÊóÖ„ÅÆÊóÖÁ®ã„ÇíË®≠Ë®à„Åô„ÇãAI„Åß„Åô„ÄÇ

# „Ç¢„É´„Ç¥„É™„Ç∫„É†
1. **Êã†ÁÇπÊà¶Áï•:** „É¶„Éº„Ç∂„ÉºÊåáÂÆö„ÅÆ„ÄåÊã†ÁÇπ(base_area)„Äç„Çí„Çπ„Çø„Éº„ÉàÂú∞ÁÇπ„Å®„Åô„Çã
2. **„É´„Éº„ÉàÊúÄÈÅ©Âåñ:** Êã†ÁÇπ ‚Üí „É°„Ç∏„É£„Éº„Çπ„Éù„ÉÉ„Éà ‚Üí „Çµ„ÉÜ„É©„Ç§„ÉàÔºàÁ©¥Â†¥Ôºâ ‚Üí Êã†ÁÇπ „Å∏Êàª„Çã„Äå‰∏ÄÁ≠ÜÊõ∏„Åç„É´„Éº„Éà„Äç„ÇíÊßãÁØâ„Åô„Çã
3. **ÊôÇÈñìÁÆ°ÁêÜ:** ÂêÑÂú∞ÁÇπÈñì„ÅÆÁßªÂãïÊôÇÈñì„ÇíËÄÉÊÖÆ„Åó„Å¶„ÄÅÁèæÂÆüÁöÑ„Å™„Çπ„Ç±„Ç∏„É•„Éº„É´„ÇíÁµÑ„ÇÄ
4. **È£ü‰∫ã:** ÁâπÂÆö„ÅÆÂ∫ó„Çí‰∫àÁ¥Ñ„Åï„Åõ„Å™„ÅÑ„ÄÇ„Äå„Åì„ÅÆ„Ç®„É™„Ç¢„Å™„Çâ‚óã‚óã„Åå„Åä„Åô„Åô„ÇÅÔºàÂÄôË£ú: AÂ∫ó, BÂ∫óÔºâ„Äç„Å®„ÅÑ„ÅÜÊèêÊ°à„Å´Áïô„ÇÅ„Çã

# Google Maps URLÁîüÊàêÔºàÈáçË¶ÅÔºâ
ÂêÑÊó•„ÅÆ„É´„Éº„Éà„Å´ÂØæ„Åó„Å¶„ÄÅÂÆüÈöõ„Å´Ê©üËÉΩ„Åô„ÇãGoogle Maps URL„ÇíÁîüÊàê„Åô„Çã„Åì„Å®„ÄÇ

**„Éï„Ç©„Éº„Éû„ÉÉ„Éà:**
\`https://www.google.com/maps/dir/?api=1&origin={Êã†ÁÇπ„ÅÆURL„Ç®„É≥„Ç≥„Éº„ÉâÊ∏à„ÅøÂêç}&destination={Êã†ÁÇπ„ÅÆURL„Ç®„É≥„Ç≥„Éº„ÉâÊ∏à„ÅøÂêç}&waypoints={„Çπ„Éù„ÉÉ„ÉàA}|{„Çπ„Éù„ÉÉ„ÉàB}|{„Çπ„Éù„ÉÉ„ÉàC}\`

**„É´„Éº„É´:**
- originÔºàÂá∫Áô∫Âú∞Ôºâ„Å®destinationÔºàÂà∞ÁùÄÂú∞Ôºâ„ÅØ‰∏°Êñπ„Å®„ÇÇÊã†ÁÇπ„Ç®„É™„Ç¢Ôºàbase_areaÔºâ„Å´„Åô„Çã
- waypoints„ÅØ„Äå|„ÄçÔºà„Éë„Ç§„ÉóÔºâ„ÅßÂå∫Âàá„Çã
- Êó•Êú¨Ë™û„ÅÆ„Çπ„Éù„ÉÉ„ÉàÂêç„ÅØ„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî®ÂèØËÉΩÔºà„Éñ„É©„Ç¶„Ç∂„ÅåËá™Âãï„Ç®„É≥„Ç≥„Éº„ÉâÔºâ
- ‰æã: \`https://www.google.com/maps/dir/?api=1&origin=Èï∑Â¥éÈßÖ&destination=Èï∑Â¥éÈßÖ&waypoints=„Ç∞„É©„Éê„ÉºÂúí|Â§ßÊµ¶Â§©‰∏ªÂ†Ç|Âá∫Â≥∂\`

# ÁßªÂãïÊâãÊÆµ„ÅÆËÄÉÊÖÆ
- **carÔºàËªäÔºâ„ÅÆÂ†¥Âêà:** ÈßêËªäÂ†¥„ÅÆÊúâÁÑ°„ÇíËÄÉÊÖÆ„ÄÅËªä„Åß„Ç¢„ÇØ„Çª„Çπ„Åó„ÇÑ„Åô„ÅÑ„É´„Éº„Éà„ÇíÂÑ™ÂÖà
- **transitÔºàÂÖ¨ÂÖ±‰∫§ÈÄöÔºâ„ÅÆÂ†¥Âêà:** ÈßÖ„Éª„Éê„ÇπÂÅú„Åã„Çâ„ÅÆ„Ç¢„ÇØ„Çª„Çπ„ÇíÈáçË¶ñ„ÄÅ‰πó„ÇäÊèõ„Åà„ÇíÊúÄÂ∞èÂåñ

# Âá∫ÂäõË®ÄË™û
**„Åô„Åπ„Å¶„ÅÆÂá∫Âäõ„ÅØÊó•Êú¨Ë™û„ÅßË®òËø∞„Åô„Çã„Åì„Å®**

# Âá∫ÂäõÊßãÈÄ†
- **title:** ÊóÖ„ÅÆ„Çø„Ç§„Éà„É´Ôºà‰æã: „ÄåÈï∑Â¥é„Éª‰Ωê‰∏ñ‰øù ÊπæÂ≤∏„Éâ„É©„Ç§„ÉñÂë®ÈÅä„ÄçÔºâ
- **intro:** ÂäπÁéáÊÄß„Å®Ëá™Áî±Â∫¶„Çí„Ç¢„Éî„Éº„É´„Åô„ÇãÂ∞éÂÖ•ÊñáÔºà100-150ÊñáÂ≠óÔºâ
- **target:** Â∏∏„Å´ "general"
- **itinerary:** Êó•„Åî„Å®„ÅÆÊóÖÁ®ã
  - day: Êó•Êï∞Ôºà1„Åã„ÇâÈñãÂßãÔºâ
  - google_maps_url: „Åù„ÅÆÊó•„ÅÆ„É´„Éº„ÉàÂÖ®‰Ωì„ÇíÁ§∫„ÅôGoogle Maps URL
  - events: „Ç§„Éô„É≥„Éà„ÅÆÈÖçÂàó
    - time: ÊôÇÂàªÔºà‰æã: "10:00"Ôºâ
    - spot: „Çπ„Éù„ÉÉ„ÉàÂêç
    - query: Google MapsÊ§úÁ¥¢„ÇØ„Ç®„É™
    - description: „Åù„ÅÆ„Çπ„Éù„ÉÉ„Éà„Åß„ÅÆÈÅé„Åî„ÅóÊñπ„ÇÑ„Éù„Ç§„É≥„Éà
    - type: "spot" | "food" | "move"
- **affiliate:** „Åä„Åô„Åô„ÇÅ„Çµ„Éº„Éì„Çπ/ÂïÜÂìÅ
  - label: Ë°®Á§∫„É©„Éô„É´
  - url: „É™„É≥„ÇØURLÔºà„É¨„É≥„Çø„Ç´„Éº„ÄÅ„Éõ„ÉÜ„É´‰∫àÁ¥Ñ„Çµ„Ç§„Éà„Å™„Å©Ôºâ`

    const transportLabel =
      input.transportation === 'car' ? 'Ëªä' : 'ÂÖ¨ÂÖ±‰∫§ÈÄöÊ©üÈñ¢'
    const userPrompt = `‰ª•‰∏ã„ÅÆÊù°‰ª∂„ÅßÊúÄÈÅ©Âåñ„Åï„Çå„ÅüÊóÖË°å„Éó„É©„É≥„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö

**ÁõÆÁöÑÂú∞:** ${input.destination}
**Êã†ÁÇπ„Ç®„É™„Ç¢:** ${input.base_area}
**ÁßªÂãïÊâãÊÆµ:** ${transportLabel}

Êã†ÁÇπ„ÇíËµ∑ÁÇπ„ÉªÁµÇÁÇπ„Å®„Åô„ÇãÂäπÁéáÁöÑ„Å™Âë®ÈÅä„É´„Éº„Éà„ÇíË®≠Ë®à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
ÂêÑÊó•„ÅÆgoogle_maps_url„Å´„ÅØ„ÄÅÂÆüÈöõ„Å´„ÇØ„É™„ÉÉ„ÇØ„Åó„Å¶‰Ωø„Åà„ÇãÊ≠£„Åó„ÅÑURL„ÇíÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`

    // Use AI SDK's streamObject for immediate partial object streaming (real-time rendering)
    console.log('[Timing] Starting LLM API call...')
    console.log('[Debug] Model:', llmClient.getModelName())
    console.log('[Debug] System prompt length:', systemPrompt.length, 'chars')
    console.log('[Debug] User prompt length:', userPrompt.length, 'chars')
    const startTime = Date.now()

    const result = streamObject({
      model: llmClient.getModel(),
      system: systemPrompt,
      prompt: userPrompt,
      schema: OptimizedPlanSchema,
      onFinish: ({ object, usage }) => {
        const duration = Date.now() - startTime

        // === [DeepDive] Performance Metrics ===
        console.log('[DeepDive] üèÅ Stream Completed')
        console.log(
          `[DeepDive] Duration: ${duration}ms (${(duration / 1000).toFixed(2)}s)`
        )

        // Token usage and TPS calculation
        if (usage) {
          const inputTokens = usage.inputTokens || 0
          const outputTokens = usage.outputTokens || 0
          const totalTokens = inputTokens + outputTokens
          const tps =
            outputTokens > 0
              ? (outputTokens / (duration / 1000)).toFixed(2)
              : 'N/A'

          console.log(`[DeepDive] Token Usage:`)
          console.log(`[DeepDive]   - Input: ${inputTokens} tokens`)
          console.log(`[DeepDive]   - Output: ${outputTokens} tokens`)
          console.log(`[DeepDive]   - Total: ${totalTokens} tokens`)
          console.log(`[DeepDive] TPS (Tokens Per Second): ${tps} tokens/sec`)

          // Token efficiency
          if (outputTokens > 0 && duration > 0) {
            const msPerToken = (duration / outputTokens).toFixed(2)
            console.log(
              `[DeepDive] Generation Speed: ${msPerToken}ms per token`
            )
          }
        } else {
          console.log('[DeepDive] ‚ö†Ô∏è No usage data available')
        }

        // === Object Output Analysis (Optimized Plan) ===
        if (object) {
          console.log('[DeepDive] üì¶ Generated Optimized Plan Analysis:')
          console.log(
            `[DeepDive]   - Top-level keys: ${Object.keys(object).join(', ')}`
          )
          if (object.title) {
            console.log(`[DeepDive]   - Title: "${object.title}"`)
          }
          if (object.itinerary) {
            console.log(
              `[DeepDive]   - Itinerary days: ${object.itinerary.length}`
            )
            object.itinerary.forEach((day, i) => {
              console.log(
                `[DeepDive]     Day ${i + 1}: ${day.events?.length || 0} events`
              )
            })
          }
          if (object.affiliate) {
            console.log(`[DeepDive]   - Affiliate: "${object.affiliate.label}"`)
          }
        } else {
          console.log('[DeepDive] ‚ö†Ô∏è No object generated')
        }

        // === Legacy logs (for compatibility) ===
        console.log(
          `[Timing] ‚úÖ Stream completed in ${duration}ms (${(duration / 1000).toFixed(2)}s)`
        )
        if (usage) {
          console.log(
            `[Token Usage] Input: ${usage.inputTokens || 0}, Output: ${usage.outputTokens || 0}, Total: ${(usage.inputTokens || 0) + (usage.outputTokens || 0)}`
          )
        }
        if (object) {
          console.log(
            `[Object Summary] Generated optimized plan: "${object.title || 'Unknown'}"`
          )
        }
      },
    })

    console.log(
      `[Timing] streamObject created in ${Date.now() - startTime}ms (note: streaming starts async)`
    )

    // Return streaming response without saving
    // Client will call POST /api/plans to save the plan after receiving it
    // CRITICAL: Pass headers to prevent Vercel compression and enable true streaming
    return result.toTextStreamResponse({
      headers: {
        'Content-Type': 'text/event-stream; charset=utf-8',
        'Cache-Control': 'no-cache, no-transform',
        'X-Content-Type-Options': 'nosniff',
        'X-Accel-Buffering': 'no', // Disable nginx buffering
      },
    })
  } catch (error) {
    console.error('Error generating plan:', error)

    if (error instanceof Error && error.message.includes('Rate limit')) {
      return new Response(JSON.stringify({ error: error.message }), {
        status: 429,
        headers: { 'Content-Type': 'application/json' },
      })
    }

    return new Response(
      JSON.stringify({
        error: 'Failed to generate plan',
        details: error instanceof Error ? error.message : 'Unknown error',
      }),
      {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      }
    )
  }
}
